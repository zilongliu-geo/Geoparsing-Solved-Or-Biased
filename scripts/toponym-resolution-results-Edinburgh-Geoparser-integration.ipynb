{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b2e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.dom.minidom\n",
    "import xml.etree.cElementTree as et\n",
    "import os\n",
    "from geopy.distance import great_circle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b260464",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_geoparsed_results = os.path.dirname(os.getcwd())+'\\\\geoparsed-results'\n",
    "lgl_dir = dir_geoparsed_results+'\\\\lgl_geocoded_results_Edinburgh_Geoparser\\\\'\n",
    "geovirus_dir = dir_geoparsed_results+'\\\\geovirus_geocoded_results_Edinburgh_Geoparser\\\\'\n",
    "wiktor_dir = dir_geoparsed_results+'\\\\wiktor_geocoded_results_Edinburgh_Geoparser\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ca3f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to read output xmls produced by Edinburgh Geoparser\n",
    "def getGeoparsedResults_lgl(index):\n",
    "    parsed_xml_out = et.parse(lgl_dir+str(index)+'.out.xml')\n",
    "    root = list(parsed_xml_out.getroot())\n",
    "    geoparsed_list = []\n",
    "    for ent in root[2].find('ents'):\n",
    "        try:\n",
    "            lat = float(ent.attrib['lat'])\n",
    "            lon = float(ent.attrib['long'])\n",
    "            toponym = ent.find('parts').find('part').text\n",
    "            geoparsed_list.append({'name':toponym,'lat':lat,'lon':lon})\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return geoparsed_list\n",
    "\n",
    "def getGeoparsedResults_geovirus(index):\n",
    "    parsed_xml_out = et.parse(geovirus_dir+str(index)+'.out.xml')\n",
    "    root = list(parsed_xml_out.getroot())\n",
    "    geoparsed_list = []\n",
    "    for ent in root[2].find('ents'):\n",
    "        try:\n",
    "            lat = float(ent.attrib['lat'])\n",
    "            lon = float(ent.attrib['long'])\n",
    "            toponym = ent.find('parts').find('part').text\n",
    "            geoparsed_list.append({'name':toponym,'lat':lat,'lon':lon})\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return geoparsed_list\n",
    "\n",
    "def getGeoparsedResults_wiktor(index):\n",
    "    parsed_xml_out = et.parse(wiktor_dir+str(index)+'.out.xml')\n",
    "    root = list(parsed_xml_out.getroot())\n",
    "    geoparsed_list = []\n",
    "    for ent in root[2].find('ents'):\n",
    "        try:\n",
    "            lat = float(ent.attrib['lat'])\n",
    "            lon = float(ent.attrib['long'])\n",
    "            toponym = ent.find('parts').find('part').text\n",
    "            geoparsed_list.append({'name':toponym,'lat':lat,'lon':lon})\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return geoparsed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8241af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to calculate keys for annotated locations in data patches\n",
    "def calc_mlg_key(toponym, old_coord):\n",
    "    toponym = toponym.strip(' ')\n",
    "    old_coord_split = old_coord.split(',')\n",
    "    old_lat = old_coord_split[0].strip(' ')\n",
    "    old_lon = old_coord_split[1].strip(' ')\n",
    "    return toponym+' '+old_lat+','+old_lon\n",
    "\n",
    "def calc_lgl_key(name, lat, lon):\n",
    "    name = name.strip(' ')\n",
    "    if (lat != None) & (lon != None):\n",
    "        lat = lat.strip(' ')\n",
    "        lon = lon.strip(' ')\n",
    "        return name+' '+lat+','+lon\n",
    "    \n",
    "def calc_geovirus_key(name, lat, lon):\n",
    "    name = name.strip(' ')\n",
    "    if (lat != None) & (lon != None):\n",
    "        lat = lat.strip(' ')\n",
    "        lon = lon.strip(' ')\n",
    "        return name+' '+lat+','+lon\n",
    "    \n",
    "def calc_wiktor_key(name, lat, lon):\n",
    "    name = name.strip(' ')\n",
    "    if (lat != None) & (lon != None):\n",
    "        lat = lat.strip(' ')\n",
    "        lon = lon.strip(' ')\n",
    "        return name+' '+lat+','+lon\n",
    "\n",
    "## Function to calculate median error distances\n",
    "def calc_median_error_distance(new_coord, geocoded_coordinates_list):\n",
    "    errors = []\n",
    "    new_coord = new_coord.split(',')\n",
    "    lat = float(new_coord[0].strip(' '))\n",
    "    lon = float(new_coord[1].strip(' '))\n",
    "    for (geoparsed_lat, geoparsed_lon) in geocoded_coordinates_list:\n",
    "        if (geoparsed_lat, geoparsed_lon) != (None, None):\n",
    "            errors.append(great_circle((float(lat),float(lon)), (float(geoparsed_lat), float(geoparsed_lon))).km)\n",
    "    if len(errors) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return np.median(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756b99b2",
   "metadata": {},
   "source": [
    "## Integrate Edinburgh Geoparser's Geoparsing Results on LGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c198c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read and preprocess LGL\n",
    "dir_lgl = os.path.dirname(os.getcwd())+'\\\\data\\\\evaluation-corpora\\\\original-datasets\\\\lgl.xml'\n",
    "parsed_xml_lgl = et.parse(dir_lgl)\n",
    "\n",
    "url_list = []\n",
    "text_list = []\n",
    "toponyms_list = []\n",
    "index_list = []\n",
    "i = 0\n",
    "for article in parsed_xml_lgl.getroot():\n",
    "    i += 1\n",
    "    url = article.find('url')\n",
    "    text = article.find('text')\n",
    "    toponyms = article.find('toponyms')\n",
    "    \n",
    "    toponym_list = []\n",
    "    for toponym in toponyms:\n",
    "        start = toponym.find('start')\n",
    "        end = toponym.find('end')\n",
    "        gaztag = toponym.find('gaztag')\n",
    "        try:\n",
    "            name = gaztag.find('name')\n",
    "            lat = gaztag.find('lat')\n",
    "            lon = gaztag.find('lon')\n",
    "            toponym_list.append({'name':name.text, 'start':start.text, 'end': end.text, 'lat': lat.text, 'lon': lon.text})    \n",
    "        except AttributeError:\n",
    "            name = toponym.find('phrase')\n",
    "            toponym_list.append({'name':name.text, 'start':start.text, 'end': end.text, 'lat': None, 'lon': None})    \n",
    "    \n",
    "    url_list.append(url.text)\n",
    "    text_list.append(text.text)\n",
    "    toponyms_list.append(toponym_list)\n",
    "    index_list.append(i)\n",
    "    \n",
    "df_lgl = pd.DataFrame({'url' :url.text, 'text': text_list, 'toponyms': toponyms_list,'index':index_list})\n",
    "\n",
    "## extract annotated locations from LGL\n",
    "name_list = []\n",
    "lat_list = []\n",
    "lon_list = []\n",
    "\n",
    "for article in parsed_xml_lgl.getroot():\n",
    "    toponyms = article.find('toponyms')\n",
    "    for toponym in toponyms:\n",
    "        gaztag = toponym.find('gaztag')\n",
    "        try:\n",
    "            name = gaztag.find('name')\n",
    "            lat = gaztag.find('lat')\n",
    "            lon = gaztag.find('lon')\n",
    "            name_list.append(name.text)\n",
    "            lat_list.append(lat.text)\n",
    "            lon_list.append(lon.text)\n",
    "        except AttributeError:\n",
    "            name = toponym.find('phrase')\n",
    "            name_list.append(name.text)\n",
    "            lat_list.append(None)\n",
    "            lon_list.append(None)\n",
    "\n",
    "df_lgl_poi = pd.DataFrame({'name' :name_list, 'lat': lat_list, 'lon': lon_list})\n",
    "\n",
    "df_lgl_poi = df_lgl_poi.drop_duplicates()\n",
    "\n",
    "## read LGL data patches\n",
    "dir_mlg_lgl = os.path.dirname(os.getcwd())+'\\\\data\\\\evaluation-corpora\\\\data-patches\\\\lgl_patches.tsv'\n",
    "mlg_lgl = pd.read_csv(dir_mlg_lgl,sep = '\\t', header = None)\n",
    "\n",
    "mlg_lgl = mlg_lgl.rename(columns = {0:'toponym', 1:'old_coord', 2:'new_coord'})\n",
    "\n",
    "mlg_lgl['mlg_key'] = mlg_lgl.apply(lambda x: calc_mlg_key(x['toponym'],x['old_coord']),axis = 1)\n",
    "\n",
    "df_lgl_poi['lgl_key'] = df_lgl_poi.apply(lambda x: calc_lgl_key(x['name'], x['lat'], x['lon']),axis = 1)\n",
    "\n",
    "## unifying LGL\n",
    "df_lgl_poi_unified = pd.merge(df_lgl_poi, mlg_lgl, how = 'outer', left_on = 'lgl_key', right_on = 'mlg_key')\n",
    "\n",
    "df_lgl_poi_unified= df_lgl_poi_unified.dropna(subset=['new_coord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33120774",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get geoparsed results of every article in LGL\n",
    "df_lgl['geoparsed_result'] = df_lgl['index'].apply(lambda index: getGeoparsedResults_lgl(index))\n",
    "\n",
    "df_lgl_poi_unified['geocoded_coordinates_list'] = df_lgl_poi_unified['name'].apply(lambda x: [])\n",
    "\n",
    "for i in range(len(df_lgl)):\n",
    "    toponyms = df_lgl['toponyms'].iloc[i]\n",
    "    geoparsed_result = df_lgl['geoparsed_result'].iloc[i]\n",
    "    for toponym in toponyms:\n",
    "        try:\n",
    "            df_lgl_poi_toponym_index = df_lgl_poi_unified[(df_lgl_poi_unified['name'] == toponym['name']) & (df_lgl_poi_unified['lat'] == toponym['lat']) & (df_lgl_poi_unified['lon'] == toponym['lon'])].index[0]\n",
    "        except IndexError:\n",
    "            continue ## no coordinate information for this annotated toponym in LGL\n",
    "        for geoparsed_toponym in geoparsed_result:\n",
    "            if (toponym['name'] == geoparsed_toponym['name']):\n",
    "                ##if (int(toponym['start']) == int(geoparsed_toponym['start'])) & (int(toponym['end']) == int(geoparsed_toponym['end'])):\n",
    "                    toponym['geoparsed_lat'] = geoparsed_toponym['lat']\n",
    "                    toponym['geoparsed_lon'] = geoparsed_toponym['lon']\n",
    "                    break\n",
    "        try:\n",
    "            df_lgl_poi_unified['geocoded_coordinates_list'][df_lgl_poi_toponym_index].append((toponym['geoparsed_lat'],toponym['geoparsed_lon']))\n",
    "        except KeyError:\n",
    "            continue ## no coordinate information for this annotated toponym in GeoNames\n",
    "\n",
    "## MdnED calculation\n",
    "df_lgl_poi_unified['median_error_distance'] = df_lgl_poi_unified.apply(lambda x: calc_median_error_distance(x['new_coord'], x['geocoded_coordinates_list']), axis = 1)\n",
    "\n",
    "## save toponym resolution results\n",
    "dir_lgl_results = os.path.dirname(os.getcwd())+'\\\\geoparsed-results'\n",
    "df_lgl_poi_unified.to_csv(dir_lgl_results+'\\\\lgl_geocoded_results_Edinburgh_Geoparser.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb8539f",
   "metadata": {},
   "source": [
    "## Integrate Edinburgh Geoparser's Geoparsing Results on GeoVirus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dcc629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read and preprocess GeoVirus\n",
    "dir_geovirus = os.path.dirname(os.getcwd())+'\\\\data\\\\evaluation-corpora\\\\original-datasets\\\\GeoVirus.xml'\n",
    "parsed_xml_geovirus = et.parse(dir_geovirus)\n",
    "\n",
    "source_list = []\n",
    "text_list = []\n",
    "locations_list = []\n",
    "index_list = []\n",
    "\n",
    "i = 0\n",
    "for article in parsed_xml_geovirus.getroot():\n",
    "    i += 1\n",
    "    source = article.find('source')\n",
    "    text = article.find('text')\n",
    "    locations = article.find('locations')\n",
    "    location_list = []\n",
    "    for location in locations:\n",
    "        name = location.find('name')\n",
    "        start = location.find('start')\n",
    "        end = location.find('end')\n",
    "        lat = location.find('lat')\n",
    "        lon = location.find('lon')\n",
    "        page = location.find('page')\n",
    "        \n",
    "        page_split = page.text.split('/')\n",
    "        wikipedia_name = page_split[len(page_split)-1]\n",
    "        wikipedia_name = wikipedia_name.replace(\"_\",\" \")\n",
    "        wikipedia_name = wikipedia_name.replace(\"%27\",\"\\'\")\n",
    "        \n",
    "        location_list.append({'name':name.text, 'wikipedia_name':wikipedia_name, 'start':start.text, 'end': end.text, 'lat': lat.text, 'lon': lon.text, 'page': page.text})    \n",
    "    \n",
    "    source_list.append(source.text)\n",
    "    text_list.append(text.text)\n",
    "    locations_list.append(location_list)\n",
    "    index_list.append(i)\n",
    "    \n",
    "\n",
    "df_geovirus = pd.DataFrame({'source' :source_list, 'text': text_list, 'locations': locations_list, 'index':index_list})\n",
    "\n",
    "## extract annotated locations from GeoVirus\n",
    "name_list = []\n",
    "lat_list = []\n",
    "lon_list = []\n",
    "page_list = []\n",
    "wikipedia_name_list = []\n",
    "\n",
    "for article in parsed_xml_geovirus.getroot():\n",
    "    locations = article.find('locations')\n",
    "    for location in locations:\n",
    "        name = location.find('name')\n",
    "        lat = location.find('lat')\n",
    "        lon = location.find('lon')\n",
    "        page = location.find('page')\n",
    "        \n",
    "        page_split = page.text.split('/')\n",
    "        wikipedia_name = page_split[len(page_split)-1]\n",
    "        wikipedia_name = wikipedia_name.replace(\"_\",\" \")\n",
    "        wikipedia_name = wikipedia_name.replace(\"%27\",\"\\'\")\n",
    "        \n",
    "        name_list.append(name.text)\n",
    "        lat_list.append(lat.text)\n",
    "        lon_list.append(lon.text)\n",
    "        page_list.append(page.text)\n",
    "        wikipedia_name_list.append(wikipedia_name)\n",
    "\n",
    "df_geovirus_poi = pd.DataFrame({'name' :name_list, 'wikipedia_name':wikipedia_name_list, 'lat': lat_list, 'lon': lon_list, 'page': page_list})\n",
    "\n",
    "df_geovirus_poi = df_geovirus_poi.drop_duplicates()\n",
    "\n",
    "## read GeoVirus data patches\n",
    "dir_mlg_geovirus = os.path.dirname(os.getcwd())+'\\\\data\\\\evaluation-corpora\\\\data-patches\\\\GeoVirus_patches.tsv'\n",
    "mlg_geovirus = pd.read_csv(dir_mlg_geovirus,sep = '\\t', header = None)\n",
    "\n",
    "mlg_geovirus = mlg_geovirus.rename(columns = {0:'toponym', 1:'old_coord', 2:'new_coord'})\n",
    "\n",
    "mlg_geovirus['mlg_key'] = mlg_geovirus.apply(lambda x: calc_mlg_key(x['toponym'],x['old_coord']),axis = 1)\n",
    "\n",
    "df_geovirus_poi['geovirus_key'] = df_geovirus_poi.apply(lambda x: calc_geovirus_key(x['wikipedia_name'], x['lat'], x['lon']),axis = 1)\n",
    "\n",
    "## unifying GeoVirus\n",
    "df_geovirus_poi_unified = pd.merge(df_geovirus_poi, mlg_geovirus, how = 'outer', left_on = 'geovirus_key', right_on = 'mlg_key')\n",
    "\n",
    "df_geovirus_poi_unified= df_geovirus_poi_unified.dropna(subset=['new_coord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19aa6950",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get geoparsed results of every article in GeoVirus\n",
    "df_geovirus['geoparsed_result'] = df_geovirus['index'].apply(lambda index: getGeoparsedResults_geovirus(index))\n",
    "    \n",
    "df_geovirus_poi_unified['geocoded_coordinates_list'] = df_geovirus_poi_unified['name'].apply(lambda x: [])\n",
    "\n",
    "for i in range(len(df_geovirus)):\n",
    "    toponyms = df_geovirus['locations'].iloc[i]\n",
    "    geoparsed_result = df_geovirus['geoparsed_result'].iloc[i]\n",
    "    for toponym in toponyms:\n",
    "        try:\n",
    "            df_geovirus_poi_toponym_index = df_geovirus_poi_unified[(df_geovirus_poi_unified['name'] == toponym['name']) & (df_geovirus_poi_unified['lat'] == toponym['lat']) & (df_geovirus_poi_unified['lon'] == toponym['lon'])].index[0]\n",
    "        except IndexError:\n",
    "            continue ## no coordinate information for this annotated toponym in geovirus\n",
    "        for geoparsed_toponym in geoparsed_result:\n",
    "            if (toponym['name'] == geoparsed_toponym['name']):\n",
    "                ##if (int(toponym['start']) == int(geoparsed_toponym['start'])) & (int(toponym['end']) == int(geoparsed_toponym['end'])):\n",
    "                    toponym['geoparsed_lat'] = geoparsed_toponym['lat']\n",
    "                    toponym['geoparsed_lon'] = geoparsed_toponym['lon']\n",
    "                    break\n",
    "        try:\n",
    "            df_geovirus_poi_unified['geocoded_coordinates_list'][df_geovirus_poi_toponym_index].append((toponym['geoparsed_lat'],toponym['geoparsed_lon']))\n",
    "        except KeyError:\n",
    "            continue ## no coordinate information for this annotated toponym in GeoNames\n",
    "\n",
    "## MdnED calculation            \n",
    "df_geovirus_poi_unified['median_error_distance'] = df_geovirus_poi_unified.apply(lambda x: calc_median_error_distance(x['new_coord'], x['geocoded_coordinates_list']), axis = 1)\n",
    "\n",
    "## save toponym resolution results\n",
    "dir_geovirus_results = os.path.dirname(os.getcwd())+'\\\\geoparsed-results'\n",
    "df_geovirus_poi_unified.to_csv(dir_geovirus_results+'\\\\geovirus_geocoded_results_Edinburgh_Geoparser.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab3b1b",
   "metadata": {},
   "source": [
    "## Integrate Edinburgh Geoparser's Geoparsing Results on WikToR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45f85368",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read and preprocess WikToR\n",
    "dir_wiktor = os.path.dirname(os.getcwd())+'\\\\data\\\\evaluation-corpora\\\\original-datasets\\\\WikToR.xml'\n",
    "parsed_xml_wiktor = et.parse(dir_wiktor)\n",
    "\n",
    "url_list = []\n",
    "text_list = []\n",
    "toponyms_list = []\n",
    "index_list = []\n",
    "\n",
    "i = 0\n",
    "for page in parsed_xml_wiktor.getroot():\n",
    "    i += 1\n",
    "    url = page.find('url')\n",
    "    text = page.find('text')\n",
    "    name = page.find('toponymName')\n",
    "    wikipedia_name = page.find('pageTitle')\n",
    "    lat = page.find('lat')\n",
    "    lon = page.find('lon')\n",
    "\n",
    "    toponyms = page.find('toponymIndices')\n",
    "    toponym_list = []\n",
    "    for toponym in toponyms:\n",
    "        start = toponym.find('start')\n",
    "        end = toponym.find('end')        \n",
    "        toponym_list.append({'name':name.text, 'wikipedia_name':wikipedia_name.text, 'start':start.text, 'end': end.text, 'lat': lat.text, 'lon': lon.text, 'page': url.text})    \n",
    "    \n",
    "    url_list.append(url.text)\n",
    "    text_list.append(text.text)\n",
    "    toponyms_list.append(toponym_list)\n",
    "    index_list.append(i)\n",
    "    \n",
    "\n",
    "df_wiktor = pd.DataFrame({'url' :url_list, 'text': text_list, 'toponyms': toponyms_list, 'index':index_list})\n",
    "\n",
    "## extract annotated locations from WikToR\n",
    "name_list = []\n",
    "lat_list = []\n",
    "lon_list = []\n",
    "page_list = []\n",
    "wikipedia_name_list = []\n",
    "\n",
    "for page in parsed_xml_wiktor.getroot():\n",
    "    url = page.find('url')\n",
    "    name = page.find('toponymName')\n",
    "    wikipedia_name = page.find('pageTitle')\n",
    "    lat = page.find('lat')\n",
    "    lon = page.find('lon')\n",
    "\n",
    "    name_list.append(name.text)\n",
    "    lat_list.append(lat.text)\n",
    "    lon_list.append(lon.text)\n",
    "    page_list.append(url.text)\n",
    "    wikipedia_name_list.append(wikipedia_name.text)\n",
    "\n",
    "df_wiktor_poi = pd.DataFrame({'name' :name_list, 'wikipedia_name':wikipedia_name_list, 'lat': lat_list, 'lon': lon_list, 'page': url_list})\n",
    "\n",
    "df_wiktor_poi = df_wiktor_poi.drop_duplicates()\n",
    "\n",
    "## read WikToR data patches\n",
    "dir_mlg_wiktor = os.path.dirname(os.getcwd())+'\\\\data\\\\evaluation-corpora\\\\data-patches\\\\WikToR_patches.tsv'\n",
    "mlg_wiktor = pd.read_csv(dir_mlg_wiktor,sep = '\\t', header = None)\n",
    "\n",
    "mlg_wiktor = mlg_wiktor.rename(columns = {0:'toponym', 1:'old_coord', 2:'new_coord'})\n",
    "\n",
    "mlg_wiktor['mlg_key'] = mlg_wiktor.apply(lambda x: calc_mlg_key(x['toponym'],x['old_coord']),axis = 1)\n",
    "\n",
    "df_wiktor_poi['wiktor_key'] = df_wiktor_poi.apply(lambda x: calc_wiktor_key(x['wikipedia_name'], x['lat'], x['lon']),axis = 1)\n",
    "\n",
    "## unifying WikToR\n",
    "df_wiktor_poi_unified = pd.merge(df_wiktor_poi, mlg_wiktor, how = 'outer', left_on = 'wiktor_key', right_on = 'mlg_key')\n",
    "\n",
    "df_wiktor_poi_unified= df_wiktor_poi_unified.dropna(subset=['new_coord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d59eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get geoparsed results of every article in WikToR\n",
    "df_wiktor['geoparsed_result'] = df_wiktor['index'].apply(lambda index: getGeoparsedResults_wiktor(index))\n",
    "    \n",
    "df_wiktor_poi_unified['geocoded_coordinates_list'] = df_wiktor_poi_unified['name'].apply(lambda x: [])\n",
    "\n",
    "for i in range(len(df_wiktor)):\n",
    "    toponyms = df_wiktor['toponyms'].iloc[i]\n",
    "    geoparsed_result = df_wiktor['geoparsed_result'].iloc[i]\n",
    "    for toponym in toponyms:\n",
    "        try:\n",
    "            df_wiktor_poi_toponym_index = df_wiktor_poi_unified[(df_wiktor_poi_unified['name'] == toponym['name']) & (df_wiktor_poi_unified['lat'] == toponym['lat']) & (df_wiktor_poi_unified['lon'] == toponym['lon'])].index[0]\n",
    "        except IndexError:\n",
    "            continue ## no coordinate information for this annotated toponym in wiktor\n",
    "        for geoparsed_toponym in geoparsed_result:\n",
    "            if (toponym['name'] == geoparsed_toponym['name']):\n",
    "                    toponym['geoparsed_lat'] = geoparsed_toponym['lat']\n",
    "                    toponym['geoparsed_lon'] = geoparsed_toponym['lon']\n",
    "                    break\n",
    "        try:\n",
    "            df_wiktor_poi_unified['geocoded_coordinates_list'][df_wiktor_poi_toponym_index].append((toponym['geoparsed_lat'],toponym['geoparsed_lon']))\n",
    "        except KeyError:\n",
    "            continue ## no coordinate information for this annotated toponym in GeoNames\n",
    "\n",
    "## MdnED calculation                        \n",
    "df_wiktor_poi_unified['median_error_distance'] = df_wiktor_poi_unified.apply(lambda x: calc_median_error_distance(x['new_coord'], x['geocoded_coordinates_list']), axis = 1)\n",
    "\n",
    "## save toponym resolution results\n",
    "dir_wiktor_results = os.path.dirname(os.getcwd())+'\\\\geoparsed-results'\n",
    "df_wiktor_poi_unified.to_csv(dir_wiktor_results+'\\\\wiktor_geocoded_results_Edinburgh_Geoparser.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a05cad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
